{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -q bitsandbytes-cuda110","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:26.144086Z","iopub.execute_input":"2022-06-25T18:59:26.144770Z","iopub.status.idle":"2022-06-25T18:59:26.149752Z","shell.execute_reply.started":"2022-06-25T18:59:26.144653Z","shell.execute_reply":"2022-06-25T18:59:26.149025Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport os\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler\nimport random\nfrom transformers import AdamW, AutoTokenizer, AutoModel, DataCollatorWithPadding, AutoConfig\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler\nfrom sklearn.model_selection import train_test_split\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport math\nfrom torch.optim import Adam, lr_scheduler\nfrom sklearn.model_selection import KFold\nimport urllib.request\nfrom typing import List\nfrom functools import partial\nfrom sklearn.model_selection import StratifiedShuffleSplit, GroupKFold, KFold\nfrom torch.cuda.amp import autocast, GradScaler\nfrom transformers.models.deberta.modeling_deberta import ContextPooler\nfrom nltk.corpus import stopwords\nimport string\n\nimport torchmetrics\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-25T18:59:26.155567Z","iopub.execute_input":"2022-06-25T18:59:26.156263Z","iopub.status.idle":"2022-06-25T18:59:28.900523Z","shell.execute_reply.started":"2022-06-25T18:59:26.156226Z","shell.execute_reply":"2022-06-25T18:59:28.899714Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/train.csv')\ntest = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:28.902348Z","iopub.execute_input":"2022-06-25T18:59:28.902867Z","iopub.status.idle":"2022-06-25T18:59:29.053057Z","shell.execute_reply.started":"2022-06-25T18:59:28.902840Z","shell.execute_reply":"2022-06-25T18:59:29.052260Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:29.054299Z","iopub.execute_input":"2022-06-25T18:59:29.054753Z","iopub.status.idle":"2022-06-25T18:59:29.071780Z","shell.execute_reply.started":"2022-06-25T18:59:29.054714Z","shell.execute_reply":"2022-06-25T18:59:29.070855Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:29.074001Z","iopub.execute_input":"2022-06-25T18:59:29.074413Z","iopub.status.idle":"2022-06-25T18:59:29.086670Z","shell.execute_reply.started":"2022-06-25T18:59:29.074375Z","shell.execute_reply":"2022-06-25T18:59:29.085746Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"seed_num = 22\nrandom.seed(seed_num)\nnp.random.seed(seed_num)\ntorch.manual_seed(seed_num)\ntorch.cuda.manual_seed_all(seed_num)\naccumulation_steps = 1","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:29.088219Z","iopub.execute_input":"2022-06-25T18:59:29.088656Z","iopub.status.idle":"2022-06-25T18:59:29.097612Z","shell.execute_reply.started":"2022-06-25T18:59:29.088612Z","shell.execute_reply":"2022-06-25T18:59:29.096540Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(\"cpu\")\n    print('No GPU available, using the CPU instead.')","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:29.098925Z","iopub.execute_input":"2022-06-25T18:59:29.099794Z","iopub.status.idle":"2022-06-25T18:59:29.138378Z","shell.execute_reply.started":"2022-06-25T18:59:29.099758Z","shell.execute_reply":"2022-06-25T18:59:29.137548Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"TRAIN_DIR = \"../input/feedback-prize-effectiveness/train\"\nTEST_DIR = \"../input/feedback-prize-effectiveness/test\"","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:29.139623Z","iopub.execute_input":"2022-06-25T18:59:29.140188Z","iopub.status.idle":"2022-06-25T18:59:29.147493Z","shell.execute_reply.started":"2022-06-25T18:59:29.140147Z","shell.execute_reply":"2022-06-25T18:59:29.146624Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_essay(essay_id):\n    essay_path = os.path.join(TRAIN_DIR, f\"{essay_id}.txt\")\n    essay_text = open(essay_path, 'r').read()\n    return essay_text","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:29.148512Z","iopub.execute_input":"2022-06-25T18:59:29.149207Z","iopub.status.idle":"2022-06-25T18:59:29.157144Z","shell.execute_reply.started":"2022-06-25T18:59:29.149168Z","shell.execute_reply":"2022-06-25T18:59:29.156433Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df['essay_text'] = df['essay_id'].apply(get_essay)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:29.158385Z","iopub.execute_input":"2022-06-25T18:59:29.158719Z","iopub.status.idle":"2022-06-25T18:59:41.656123Z","shell.execute_reply.started":"2022-06-25T18:59:29.158683Z","shell.execute_reply":"2022-06-25T18:59:41.655219Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def remove_punctuations(text):\n    for punctuation in list(string.punctuation):\n        text = text.replace(punctuation, '')\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.659434Z","iopub.execute_input":"2022-06-25T18:59:41.659848Z","iopub.status.idle":"2022-06-25T18:59:41.664839Z","shell.execute_reply.started":"2022-06-25T18:59:41.659803Z","shell.execute_reply":"2022-06-25T18:59:41.663759Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"STOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.666342Z","iopub.execute_input":"2022-06-25T18:59:41.666867Z","iopub.status.idle":"2022-06-25T18:59:41.679088Z","shell.execute_reply.started":"2022-06-25T18:59:41.666830Z","shell.execute_reply":"2022-06-25T18:59:41.678174Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.680760Z","iopub.execute_input":"2022-06-25T18:59:41.681380Z","iopub.status.idle":"2022-06-25T18:59:41.686655Z","shell.execute_reply.started":"2022-06-25T18:59:41.681341Z","shell.execute_reply":"2022-06-25T18:59:41.685618Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.688264Z","iopub.execute_input":"2022-06-25T18:59:41.688823Z","iopub.status.idle":"2022-06-25T18:59:41.696142Z","shell.execute_reply.started":"2022-06-25T18:59:41.688789Z","shell.execute_reply":"2022-06-25T18:59:41.695382Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def decontraction(phrase):\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.697415Z","iopub.execute_input":"2022-06-25T18:59:41.698043Z","iopub.status.idle":"2022-06-25T18:59:41.706648Z","shell.execute_reply.started":"2022-06-25T18:59:41.697961Z","shell.execute_reply":"2022-06-25T18:59:41.705863Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# df['essay_text'] = df['essay_text'].apply(remove_punctuations)\n# df['discourse_text'] = df['discourse_text'].apply(remove_punctuations)\n# df['essay_text'] = df['essay_text'].apply(lambda x: str(x).lower())\n# df['essay_text'] = df['essay_text'].apply(lambda x: re.sub('\\s+',  ' ', x))\n# df['essay_text'] = df['essay_text'].apply(lambda x: decontraction(x))\n# df['discourse_text'] = df['discourse_text'].apply(lambda x: str(x).lower())\n# df['discourse_text'] = df['discourse_text'].apply(lambda x: re.sub('\\s+',  ' ', x))\n# df['discourse_text'] = df['discourse_text'].apply(lambda x: decontraction(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.707927Z","iopub.execute_input":"2022-06-25T18:59:41.708467Z","iopub.status.idle":"2022-06-25T18:59:41.715495Z","shell.execute_reply.started":"2022-06-25T18:59:41.708431Z","shell.execute_reply":"2022-06-25T18:59:41.714649Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.716663Z","iopub.execute_input":"2022-06-25T18:59:41.717416Z","iopub.status.idle":"2022-06-25T18:59:41.740298Z","shell.execute_reply.started":"2022-06-25T18:59:41.717379Z","shell.execute_reply":"2022-06-25T18:59:41.739528Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"gkf = GroupKFold(n_splits=5)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.741716Z","iopub.execute_input":"2022-06-25T18:59:41.742156Z","iopub.status.idle":"2022-06-25T18:59:41.747147Z","shell.execute_reply.started":"2022-06-25T18:59:41.742119Z","shell.execute_reply":"2022-06-25T18:59:41.746233Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.748975Z","iopub.execute_input":"2022-06-25T18:59:41.749441Z","iopub.status.idle":"2022-06-25T18:59:41.755759Z","shell.execute_reply.started":"2022-06-25T18:59:41.749403Z","shell.execute_reply":"2022-06-25T18:59:41.754818Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# tokenizer.encode('hi good asdf wer qweasd', 'hi',truncation=True, max_length=6, padding='max_length', truncation_strategy='only_first')","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.757739Z","iopub.execute_input":"2022-06-25T18:59:41.758712Z","iopub.status.idle":"2022-06-25T18:59:41.765980Z","shell.execute_reply.started":"2022-06-25T18:59:41.758677Z","shell.execute_reply":"2022-06-25T18:59:41.764963Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# class FeedBackDataset(Dataset):\n#     def __init__(self, df, tokenizer, max_length):\n#         self.df = df\n#         self.max_len = max_length\n#         self.tokenizer = tokenizer\n#         self.discourse = df['discourse_text'].values\n#         self.essay = df['essay_text'].values\n#         self.targets = df['discourse_effectiveness'].values\n        \n#     def __len__(self):\n#         return len(self.df)\n    \n#     def __getitem__(self, index):\n#         discourse = self.discourse[index]\n#         essay = self.essay[index]\n#         text = discourse + \" \" + self.tokenizer.sep_token + \" \" + essay\n#         inputs = self.tokenizer.encode_plus(\n#                         text,\n#                         truncation=True,\n#                         add_special_tokens=True,\n#                         max_length=self.max_len\n#                     )\n        \n#         return {\n#             'input_ids': inputs['input_ids'],\n#             'attention_mask': inputs['attention_mask'],\n#             'target': self.targets[index]\n#         }","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.767101Z","iopub.execute_input":"2022-06-25T18:59:41.768132Z","iopub.status.idle":"2022-06-25T18:59:41.774450Z","shell.execute_reply.started":"2022-06-25T18:59:41.768103Z","shell.execute_reply":"2022-06-25T18:59:41.773549Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class FeedbackDataset(Dataset):\n    def __init__(self, data, is_train=True):\n        super().__init__()\n        self.max_length = 512\n        self.data = data\n        self.tokenizer = AutoTokenizer.from_pretrained(\"../input/deberta-v3-base/deberta-v3-base\", use_fast=True)\n        self.is_train = is_train\n    \n    def labeling(self, label):\n        new_label = {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}\n        return new_label[label]\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if self.is_train:\n            sentence1, sentence2, label = self.data['discourse_type'][idx], self.data['discourse_text'][idx], self.data['discourse_effectiveness'][idx]\n            essay = self.data['essay_text'][idx]\n            label = self.labeling(label)\n            label = torch.LongTensor([label])\n        else:\n            sentence1, sentence2 = self.data['discourse_type'][idx], self.data['discourse_text'][idx]\n            essay = self.data['essay_text'][idx]\n        text = sentence1 + \" \" + self.tokenizer.sep_token + \" \" + sentence2 + \" \" +self.tokenizer.sep_token + \" \"+essay\n        input_ids = self.tokenizer.encode(text, truncation=True, max_length=self.max_length, padding=True)\n        input_ids = torch.LongTensor(input_ids)\n        if self.is_train:\n            return {\n                'input_ids': input_ids,\n                'label': label\n            }\n        else:\n            return {\n                'input_ids': input_ids\n            }","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.775939Z","iopub.execute_input":"2022-06-25T18:59:41.776653Z","iopub.status.idle":"2022-06-25T18:59:41.789166Z","shell.execute_reply.started":"2022-06-25T18:59:41.776617Z","shell.execute_reply":"2022-06-25T18:59:41.788361Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.790467Z","iopub.execute_input":"2022-06-25T18:59:41.791425Z","iopub.status.idle":"2022-06-25T18:59:41.801287Z","shell.execute_reply.started":"2022-06-25T18:59:41.791386Z","shell.execute_reply":"2022-06-25T18:59:41.800355Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class FeedbackModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert_model = AutoModel.from_pretrained(\"../input/deberta-v3-base/deberta-v3-base\")\n        self.fc = nn.Linear(self.bert_model.config.hidden_size, 3)\n        self.bn = nn.BatchNorm1d(3)\n        self.dropout = nn.Dropout(0.3)\n#         self.pooler = ContextPooler(AutoConfig.from_pretrained(\"../input/deberta-v3-base/deberta-v3-base\"))\n        self.pooler = MeanPooling()\n\n#         self._init_params()\n        \n        \n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n        \n    \n    def forward(self, input_ids):\n        attention_mask = (input_ids!=0).long()\n        x = self.bert_model(input_ids, attention_mask=attention_mask, output_hidden_states=False)\n        x = self.pooler(x.last_hidden_state, attention_mask)\n#         x = self.pooler(x.last_hidden_state)\n        output = self.fc(self.dropout(x))\n#         output = self.fc(x)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.802246Z","iopub.execute_input":"2022-06-25T18:59:41.804225Z","iopub.status.idle":"2022-06-25T18:59:41.815181Z","shell.execute_reply.started":"2022-06-25T18:59:41.804118Z","shell.execute_reply":"2022-06-25T18:59:41.814446Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# train['discourse_effectiveness'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.816673Z","iopub.execute_input":"2022-06-25T18:59:41.817438Z","iopub.status.idle":"2022-06-25T18:59:41.821606Z","shell.execute_reply.started":"2022-06-25T18:59:41.817314Z","shell.execute_reply":"2022-06-25T18:59:41.820806Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed_num)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.823111Z","iopub.execute_input":"2022-06-25T18:59:41.823816Z","iopub.status.idle":"2022-06-25T18:59:41.830247Z","shell.execute_reply.started":"2022-06-25T18:59:41.823774Z","shell.execute_reply":"2022-06-25T18:59:41.829308Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# indices = list(range(len(df)))\n# train_idx, valid_idx = next(sss.split(indices, df['discourse_effectiveness'].tolist()))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.833024Z","iopub.execute_input":"2022-06-25T18:59:41.833623Z","iopub.status.idle":"2022-06-25T18:59:41.839127Z","shell.execute_reply.started":"2022-06-25T18:59:41.833595Z","shell.execute_reply":"2022-06-25T18:59:41.838400Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# train_df = df.iloc[train_idx]\n# valid_df = df.iloc[valid_idx]\n# train_df.reset_index(inplace=True, drop=True)\n# valid_df.reset_index(inplace=True, drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.840580Z","iopub.execute_input":"2022-06-25T18:59:41.841227Z","iopub.status.idle":"2022-06-25T18:59:41.846493Z","shell.execute_reply.started":"2022-06-25T18:59:41.841192Z","shell.execute_reply":"2022-06-25T18:59:41.845667Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# train_ds = FeedbackDataset(train_df)\n# valid_ds = FeedbackDataset(valid_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.851498Z","iopub.execute_input":"2022-06-25T18:59:41.852180Z","iopub.status.idle":"2022-06-25T18:59:41.857129Z","shell.execute_reply.started":"2022-06-25T18:59:41.852149Z","shell.execute_reply":"2022-06-25T18:59:41.856288Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"batch_size = 8\nepochs = 3\n# train_dataLoader = DataLoader(train_ds, batch_size=batch_size)\n# valid_dataLoader = DataLoader(valid_ds, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.858367Z","iopub.execute_input":"2022-06-25T18:59:41.859743Z","iopub.status.idle":"2022-06-25T18:59:41.866625Z","shell.execute_reply.started":"2022-06-25T18:59:41.859718Z","shell.execute_reply":"2022-06-25T18:59:41.865833Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def set_embedding_parameters_bits(embeddings_path, optim_bits=32):\n    \"\"\"\n    https://github.com/huggingface/transformers/issues/14819#issuecomment-1003427930\n    \"\"\"\n    \n    embedding_types = (\"word\", \"position\", \"token_type\")\n    for embedding_type in embedding_types:\n        attr_name = f\"{embedding_type}_embeddings\"\n        \n        if hasattr(embeddings_path, attr_name): \n            bnb.optim.GlobalOptimManager.get_instance().register_module_override(\n                getattr(embeddings_path, attr_name), 'weight', {'optim_bits': optim_bits}\n            )","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.868327Z","iopub.execute_input":"2022-06-25T18:59:41.868584Z","iopub.status.idle":"2022-06-25T18:59:41.877411Z","shell.execute_reply.started":"2022-06-25T18:59:41.868561Z","shell.execute_reply":"2022-06-25T18:59:41.876577Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n        self.criterion = nn.CrossEntropyLoss(reduction='none')\n\n    def forward(self, inputs, targets):\n    \n        ce_loss = self.criterion(inputs, targets)\n\n        pt = torch.exp(-ce_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.878544Z","iopub.execute_input":"2022-06-25T18:59:41.879629Z","iopub.status.idle":"2022-06-25T18:59:41.888534Z","shell.execute_reply.started":"2022-06-25T18:59:41.879584Z","shell.execute_reply":"2022-06-25T18:59:41.887539Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"criterion2 = nn.CrossEntropyLoss()\ncriterion = FocalLoss()\n# criterion = nn.CrossEntropyLoss()\n\ntrain_acc = torchmetrics.Accuracy()\ndef cal_accuracy(X,Y):\n    predict_scores = F.softmax(X, dim=1)\n    predict_labels = torch.argmax(predict_scores, dim=-1)\n    acc = train_acc(predict_labels.to('cpu'), y.cpu())\n    return acc","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.889742Z","iopub.execute_input":"2022-06-25T18:59:41.890266Z","iopub.status.idle":"2022-06-25T18:59:41.900372Z","shell.execute_reply.started":"2022-06-25T18:59:41.890220Z","shell.execute_reply":"2022-06-25T18:59:41.899461Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train_dataset = FeedbackDataset(df)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:41.901655Z","iopub.execute_input":"2022-06-25T18:59:41.902289Z","iopub.status.idle":"2022-06-25T18:59:42.604352Z","shell.execute_reply.started":"2022-06-25T18:59:41.902252Z","shell.execute_reply":"2022-06-25T18:59:42.603508Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"collate_fn = DataCollatorWithPadding(tokenizer=AutoTokenizer.from_pretrained(\"../input/deberta-v3-base/deberta-v3-base\", use_fast=True))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:42.605763Z","iopub.execute_input":"2022-06-25T18:59:42.606334Z","iopub.status.idle":"2022-06-25T18:59:43.285757Z","shell.execute_reply.started":"2022-06-25T18:59:42.606296Z","shell.execute_reply":"2022-06-25T18:59:43.284793Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"for fold,(train_idx,valid_idx) in enumerate(gkf.split(train_dataset, groups=df.essay_id)):\n#     if fold<=1:\n#         continue\n    cnt = 0\n    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n    train_dataLoader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_subsampler,  collate_fn=collate_fn, \n                              num_workers=2, pin_memory=True, drop_last=True)\n    valid_dataLoader = DataLoader(train_dataset, batch_size=batch_size*2, sampler=valid_subsampler,  collate_fn=collate_fn, \n                              num_workers=2, pin_memory=True, drop_last=True)\n    best_acc = 0\n    best_loss = 10\n    model = FeedbackModel().to(device)\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n#     optimizer = AdamW(optimizer_grouped_parameters, betas=(0.9, 0.98), lr=1e-5, eps=1e-8)\n    num_train_optimization_steps = int(epochs * len(train_dataLoader) / accumulation_steps)\n#     optimizer = AdamW(model.parameters(), lr=2e-5)\n    optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-6)\n#     scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.05 * num_train_optimization_steps,\n#                                                 num_training_steps=num_train_optimization_steps)\n    scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=500, \n                                                   eta_min=1e-6)\n#     scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n#                                                     num_training_steps=len(train_dataLoader) * epochs)\n    model.zero_grad()\n    print(f'------------fold no---------{fold + 1}----------------------')\n    scaler = GradScaler()\n    for epoch_i in range(0, epochs):\n        model.train()\n        total_loss = 0\n        train_accuracy = 0\n        nb_train_steps = 0\n        dataset_size = 0\n        running_loss = 0.0\n        bar = tqdm(enumerate(train_dataLoader), total=len(train_dataLoader))\n        for step, batch in bar:\n#             batch = tuple(t.to(device) for t in batch)\n#             input_ids, label = batch\n            input_ids = batch['input_ids'].to(device)\n            label = batch['labels'].to(device)\n            with torch.cuda.amp.autocast():\n                outputs = model(input_ids)\n                y = label.view(-1)\n                loss = criterion(outputs, y)\n            total_loss += loss.item()\n#             loss.backward()\n            scaler.scale(loss).backward()\n            if step % accumulation_steps == 0 or step == len(bar) - 1:\n#                 scaler.unscale_(optimizer)\n#                 torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                scheduler.step()\n#             optimizer.step()\n#             scaler.step(optimizer)\n#             scaler.update()\n#             optimizer.zero_grad()\n#             scheduler.step()\n            logits = outputs\n            tmp_train_accuracy = cal_accuracy(logits, label.to('cpu').numpy())\n            train_accuracy += tmp_train_accuracy\n            nb_train_steps += 1\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n            epoch_loss = running_loss / dataset_size\n            bar.set_postfix(Epoch=epoch_i, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n#         avg_train_loss = total_loss / len(train_dataLoader)\n#         print('')\n#         print(epoch_i + 1, f'  Average training loss: {avg_train_loss:.4f}')\n#         print(f'  Accuracy: {train_accuracy/(nb_train_steps):.4f}')\n        model.eval()\n        eval_loss, eval_accuracy = 0, 0\n        nb_eval_steps, nb_eval_examples = 0, 0\n        valid_loss = 0\n        dataset_size = 0\n        running_loss = 0.0\n        bar = tqdm(enumerate(valid_dataLoader), total=len(valid_dataLoader))\n        for step, batch in bar:\n            input_ids = batch['input_ids'].to(device)\n            label = batch['labels'].to(device)\n#             batch = tuple(t.to(device) for t in batch)\n#             input_ids, label = batch\n            with torch.no_grad():     \n                outputs = model(input_ids)\n            y = label.view(-1)\n            loss = criterion2(outputs, y)\n            valid_loss += loss.item()\n            logits = outputs\n            tmp_eval_accuracy = cal_accuracy(logits, label.to('cpu').numpy())\n            eval_accuracy += tmp_eval_accuracy\n            nb_eval_steps += 1\n            running_loss += (loss.item() * batch_size * 2)\n            dataset_size += batch_size * 2\n            epoch_loss = running_loss / dataset_size\n            bar.set_postfix(Epoch=epoch_i, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n        avg_valid_loss = valid_loss / len(valid_dataLoader)\n        valid_accuracy = eval_accuracy/(nb_eval_steps)\n        if best_loss > avg_valid_loss:\n            cnt=0\n            best_loss = avg_valid_loss\n            torch.save(\n            {\n                \"model\": \"FeedbackModel\",\n                \"model_state_dict\": model.state_dict(),\n                \"description\": f\"FeedbackModel 체크포인트-{fold + 1}\",\n            },\n            f\"/kaggle/working/checkpoint-{fold + 1}.pt\",\n        )\n            print(f'model{fold + 1} saved')\n        else:\n            cnt+=1\n            if cnt==2:\n                print(f'early stop {fold+1} fold, {epoch_i} epcoh_i')\n                print(epoch_i + 1, f'  Average valid loss: {avg_valid_loss:.4f}')\n                print(f'  Accuracy: {valid_accuracy:.4f}')\n                break\n        print(epoch_i + 1, f'  Average valid loss: {avg_valid_loss:.4f}')\n        print(f'  Accuracy: {valid_accuracy:.4f}')","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:59:43.287780Z","iopub.execute_input":"2022-06-25T18:59:43.288177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_loss = 10\n# best_acc = 0\n# no_decay = ['bias', 'LayerNorm.weight']\n# optimizer_grouped_parameters = [\n#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n# ]\n# optimizer = AdamW(optimizer_grouped_parameters, betas=(0.9, 0.98), lr=2e-5, eps=1e-8)\n# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n#                                                 num_training_steps=len(train_dataLoader) * epochs)\n# model.zero_grad()\n# for epoch_i in range(0, epochs):\n#     total_loss = 0\n#     total_acc = 0\n#     nb_train_steps = 0\n#     train_accuracy = 0\n#     model.train()\n#     for batch in tqdm(train_dataLoader):\n#         batch = tuple(t.to(device) for t in batch)\n#         input_ids, label = batch\n#         outputs = model(input_ids)\n#         y = label.view(-1)\n#         loss = criterion(outputs, y)\n#         total_loss += loss.item()\n#         loss.backward()\n#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n#         optimizer.step()\n#         optimizer.zero_grad()\n#         scheduler.step()\n#         logits = outputs\n#         tmp_train_accuracy = cal_accuracy(logits, label.cpu().numpy())\n#         train_accuracy += tmp_train_accuracy\n#         nb_train_steps += 1\n#         if nb_train_steps % 100 == 0 and not nb_train_steps == 0:\n#             print('step : {:>5,} of {:>5,} loss: {:.5f}'.format(nb_train_steps, len(train_dataLoader), loss.item()))\n#     avg_train_loss = total_loss / len(train_dataLoader)\n#     print('')\n#     print(epoch_i + 1, f'  Average training loss: {avg_train_loss:.4f}')\n#     print(f'  Accuracy: {train_accuracy/(nb_train_steps):.4f}')\n#     model.eval()\n#     eval_loss, eval_accuracy = 0, 0\n#     nb_eval_steps, nb_eval_examples = 0, 0\n#     valid_loss = 0\n#     for batch in tqdm(valid_dataLoader):\n#         batch = tuple(t.to(device) for t in batch)\n#         input_ids, label = batch\n#         with torch.no_grad():     \n#             outputs = model(input_ids)\n#         y = label.view(-1)\n#         loss = criterion(outputs, y)\n#         valid_loss += loss.item()\n#         logits = outputs\n#         tmp_eval_accuracy = cal_accuracy(logits, label.cpu().numpy())\n#         eval_accuracy += tmp_eval_accuracy\n#         nb_eval_steps += 1\n#     avg_valid_loss = valid_loss / len(valid_dataLoader)\n#     valid_accuracy = eval_accuracy/(nb_eval_steps)\n#     if best_acc < valid_accuracy:\n#         best_acc = valid_accuracy\n#         torch.save(\n#             {\n#                 \"model\": \"FeedbackModel\",\n#                 \"model_state_dict\": model.state_dict(),\n#                 \"description\": f\"FeedbackModel 체크포인트-{epoch_i}\",\n#             },\n#             f\"/kaggle/working/checkpoint-{epoch_i}.pt\",\n#         )\n#     print(epoch_i + 1, f'  Average valid loss: {avg_valid_loss:.4f}')\n#     print(f'  Accuracy: {valid_accuracy:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}